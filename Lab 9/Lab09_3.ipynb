{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7f79dc-c37d-4a02-8a48-2b0f31c7ad34",
   "metadata": {},
   "source": [
    "# Cats-dog classification - CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152f203-c12a-40f8-9fcd-3cd53c3dee6a",
   "metadata": {},
   "source": [
    "This dataset was originally published by Microsoft Research and was later made available through Kaggle, at https://www.kaggle.com/c/dogs-vs-cats/data.\n",
    "\n",
    "The dataset is hosted as a simple folder with filenames representing the label, so we might have to reorganize the dataset before we can use it.\n",
    "\n",
    "PyTorch provides a neat abstraction for images with ImageFolder and DataLoader. PyTorch expects that data is stored in the following folder structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7624c-319e-45ae-b513-2d6102cc6867",
   "metadata": {},
   "source": [
    "The dataset structure: <br>\n",
    "**Cat images** - /input/train/cat/cat*.jpg <br>\n",
    "**Dog Images** /input/train/dog/dog*.jpg <br>\n",
    "\n",
    "\n",
    "\n",
    "To simplify the process, we have provided an organized structure, with images suitable for PyTorch experiments, at https://www.kaggle.com/jojomoolayil/catsvsdogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81353dae-4a4f-446f-b54b-9d496aa7c62a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f08905-7a52-44d9-93d7-83428c97eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "import matplotlib.image as mpimg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62c843-1d4d-4232-857f-0c786dc69afc",
   "metadata": {},
   "source": [
    "## Check for GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af078c41-9579-45e8-8b40-cabcc8f5178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b3cb5b-993e-4313-8f1a-a42c9fd8d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"dogs-vs-cats/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b185135-507f-4c2d-8ba6-217e5b385d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_df[\"img_name\"] = os.listdir( os.path.join(new_path,\"train\",\"cat\")) #os.listdir(\"train/\")\n",
    "file_names = os.listdir( os.path.join(new_path,\"train\",\"cat\"))\n",
    "file_names.extend(os.listdir( os.path.join(new_path,\"train\",\"dog\")))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "915d1d0a-1b56-4b4b-acee-920261348c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(columns=[\"img_name\",\"label\"])\n",
    "\n",
    "# create dataframe with column img_name with files_names, and label (0 or 1)\n",
    "# if file name contains cat, label 0, otherwise 1 \n",
    "\n",
    "train_df[\"img_name\"] = ...\n",
    "\n",
    "...\n",
    "        \n",
    "        \n",
    "train_df.to_csv (r'train_csv.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23d238f4-9ba8-4025-8848-a74bfe65b6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat.5077.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat.2718.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat.10151.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.3406.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.4369.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>dog.9316.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>dog.6025.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>dog.8008.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>dog.1992.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>dog.12412.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            img_name label\n",
       "0       cat.5077.jpg     0\n",
       "1       cat.2718.jpg     0\n",
       "2      cat.10151.jpg     0\n",
       "3       cat.3406.jpg     0\n",
       "4       cat.4369.jpg     0\n",
       "...              ...   ...\n",
       "19995   dog.9316.jpg     1\n",
       "19996   dog.6025.jpg     1\n",
       "19997   dog.8008.jpg     1\n",
       "19998   dog.1992.jpg     1\n",
       "19999  dog.12412.jpg     1\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58951da1-f2ad-48f9-8d6f-f7846117a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CatsAndDogsDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = pd.read_csv(annotation_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.annotations.iloc[index, 0]\n",
    "        img = ... \n",
    "        y_label = ...\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dd373-0183-4078-9070-03ed78b37b20",
   "metadata": {},
   "source": [
    "## creating model\n",
    "\n",
    "We are using transfer learning here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cd4eeb7-8766-4c68-bab5-d8c3ea0f3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, train_CNN=False, num_classes=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        return self.sigmoid(self.dropout(self.relu(features))).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815233e-915b-45ca-87f5-351b600ce6c0",
   "metadata": {},
   "source": [
    "### Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcbb4009-17e3-4024-b315-2b8dd392d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa02ad",
   "metadata": {},
   "source": [
    "### Hyperparammeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9dc2b0a-646a-41c0-9374-9033fa3d28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.00001\n",
    "train_CNN = False\n",
    "batch_size = 32\n",
    "batch_size_validation = 128\n",
    "shuffle = True\n",
    "pin_memory = True\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f056221-676d-4ed9-8b25-4e2f5513f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb02f2",
   "metadata": {},
   "source": [
    "### Create data loader and dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b257994-60b8-4e3f-8302-1ea8fab96360",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CatsAndDogsDataset(\"dogs-vs-cats/train\",\"train_csv.csv\",transform=transform)\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset,[15000,5000])\n",
    "train_loader = DataLoader(dataset=train_set, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers,pin_memory=pin_memory)\n",
    "validation_loader = DataLoader(dataset=validation_set, shuffle=shuffle, batch_size=batch_size_validation,num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152dcc1-7c6e-4fe7-9b45-a7d7b5a0acbd",
   "metadata": {},
   "source": [
    "### Setting Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23289630-d9b9-40c1-802a-c0803f24a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /Users/atif/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for name, param in model.inception.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = train_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658383e6-add3-408a-9dab-0d4508740604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ef2b4b-df12-4dd5-a21c-23ad63446b2c",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15211bbb-d685-40e4-aaeb-910a66c49a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(20):\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            ...\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be395e52-915c-4269-8e8b-4a906aeff80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
